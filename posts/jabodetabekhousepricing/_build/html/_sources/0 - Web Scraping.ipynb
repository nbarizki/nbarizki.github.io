{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Web Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will demonstrate the ability of python `Selenium` and `BeautifulSoup` to scrap `Jabodetabek House Pricing` data from a third party website, utilizing other supporting package such as `logging` and `datetime` to improve insight about our web scraping framework. \n",
    "\n",
    "This website provide listing of offered price of used and new houses to be sold. Using this data, we can gain insight about *how home-owner in Jabodetabek Area values their house price and how each parameter associates with the offered price, i.e. how significant each of the property's aspect that majority of owners believe will drive their house price worthiness*.\n",
    "\n",
    "This notebook focuses entirely on how to web scrap, from identifying the website structure to developing a single `python object` to scrap the website, embracing simple `object-oriented programming` attributes and methods. Analysis of retrieved data will be explained in other notebook associated in this repository.\n",
    "\n",
    "Note that this web scraping program only works with this specific house pricing website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access website using `Chrome Webdriver`, one must verify that:\n",
    "\n",
    "1. `Chrome` (denotes the *fully installed version of Chrome Browser*) must be installed in the system. Identify the **installed version** of *Chrome* first.\n",
    "2. `Chrome Webdriver` must be provided, which its **version** shall match the **installed version** of `Chrome`. Further explanation on this can be found in https://chromedriver.chromium.org/downloads/version-selection. Download the matched version of `Chrome Webdriver`, the file name will be `chromedriver.exe`.\n",
    "3. Make sure that `Chrome Webdriver` can be detected while utilizing `Selenium` from python. I choose to add the `chromedriver.exe` executable file path to ChromeDriver when instantiating `webdriver.Chrome`.\n",
    "\n",
    " ![chromedriver](Scraped_Data/pic/1_chromedriver_version.png)\n",
    " *<center> Matching `Chrome` version with `Chrome Webdriver` version</center>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Web Scraping Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException  \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/jual/residensial/?place[]=dki-jakarta&place[]=bogor&place[]=depok&place[]=bekasi,bekasi&placeId[]=8de06376-49a3-4369-a01b-00085aefe766&placeId[]=dfd71096-eda2-4776-b3ca-542d8c5fb12b&placeId[]=a4a34395-ebe5-4930-9456-df327a9f484a&placeId[]=66899e8e-4896-467b-8e54-ab7c533bd616#qid~a0314d88-70ff-4b6c-b4bd-45f4c9f41d04'\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--disable-infobars')\n",
    "chrome_options.add_argument('start-maximized')\n",
    "chrome_options.add_argument('--disable-extensions')\n",
    "chrome_options.add_argument('--disable-notifications')\n",
    "prefs = {\"profile.default_content_setting_values.notifications\": 2}\n",
    "chrome_options.add_experimental_option('prefs', prefs)\n",
    "browser = webdriver.Chrome(\n",
    "    executable_path='D:\\\\chromedriver.exe',\n",
    "    options=chrome_options\n",
    "    )\n",
    "browser.implicitly_wait(10)\n",
    "browser.get(URL)\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Accessing `main entry list`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the house listings in main page is stored in `listing-container` as below snapshot. We are going to retrieve all of the listings in a single page by accessing this `Class Name`.\n",
    "\n",
    "\n",
    "![main_content](Scraped_Data/pic/2_Main_content.png)\n",
    "\n",
    "![listing_card](Scraped_Data/pic/3_Listing_card.png)\n",
    "\n",
    " *<center> Webpage structure of house listing </center>*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"27bc6ea0e55c8fecd59320e08cde192d\", element=\"d3094ef4-6cc7-4447-9f9a-ad8868efc41f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"27bc6ea0e55c8fecd59320e08cde192d\", element=\"b3c19c0a-e747-43ea-853c-8a11e806b8d2\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"27bc6ea0e55c8fecd59320e08cde192d\", element=\"854a09a0-1014-49a1-8535-318d2a670558\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"27bc6ea0e55c8fecd59320e08cde192d\", element=\"aea04bb6-799c-4e75-9dd4-315958cbf218\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"27bc6ea0e55c8fecd59320e08cde192d\", element=\"0fb9609e-e003-43bd-8371-59949fe2f638\")>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_listing_class = 'ui-organism-intersection__element.intersection-card-container'\n",
    "main_listings = browser.find_elements_by_class_name(main_listing_class)\n",
    "print(len(main_listings))\n",
    "main_listings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "House listing is located in `Listing Card`, the children of `main_listing` element above. We will access this to retrieve Navigation link to its detailed listing information.\n",
    "We are going to try to a house list as below codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.rumah123.com/properti/jakarta-selatan/hos11355709/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of using 1 listing\n",
    "listing_card_html = main_listings[2].get_attribute('outerHTML')\n",
    "soup_listing = BeautifulSoup(listing_card_html, \"html.parser\")\n",
    "\n",
    "# listing card properties\n",
    "listing_class = 'ui-organisms-card-r123-featured__middle-section__title'\n",
    "nav_link = 'https://www.rumah123.com' + soup_listing.select(('.' + listing_class))[0]['href']\n",
    "nav_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using above lines, we can collect all of the `listing_id` and `nav_link` from the main entry list, which contain more than one listing. Note that each listing has been isolated as a list of `WebElement` object so that we won't retrieve overlapped records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.rumah123.com/properti/jakarta-selatan/hos10107882/',\n",
       " 'https://www.rumah123.com/properti/jakarta-barat/hos10985513/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/hos11355709/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/hos10697193/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/hos8770206/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/hos7010634/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/hos10941017/',\n",
       " 'https://www.rumah123.com/properti/jakarta-timur/hos8602350/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/hos10059115/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/hos10849554/',\n",
       " 'https://www.rumah123.com/properti/jakarta-barat/aps2727387/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/hos11327520/',\n",
       " 'https://www.rumah123.com/properti/jakarta-barat/aps2726460/',\n",
       " 'https://www.rumah123.com/properti/jakarta-utara/hos2202843/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/hos11170878/',\n",
       " 'https://www.rumah123.com/properti/jakarta-utara/hos11344691/',\n",
       " 'https://www.rumah123.com/properti/jakarta-utara/hos11351396/',\n",
       " 'https://www.rumah123.com/properti/jakarta-barat/hos11352875/',\n",
       " 'https://www.rumah123.com/properti/jakarta-selatan/aps2697669/']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav_links = []\n",
    "\n",
    "for listing in main_listings:\n",
    "    try: \n",
    "        soup = BeautifulSoup(listing.get_attribute('outerHTML'))\n",
    "        listing_class = 'ui-organisms-card-r123-featured__middle-section__title'\n",
    "        nav_links.append(\n",
    "            'https://www.rumah123.com' + soup.select(('.' + listing_class))[0]['href']\n",
    "        )\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "nav_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our scraping framework will loop-accessing all of the stored navigation link of house listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Accessing specific `House Listing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having information of each listing navigation link, we now begin to scrap information of each listing by repetitively accessing each of the link using our `Chrome Webdriver`.\n",
    "\n",
    "We have to identify each particular object that we want to scrap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Listing Header**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![header](Scraped_Data/pic/4_Header.png)\n",
    "\n",
    " *<center> Listing Header </center>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information:\n",
    "\n",
    "- Price\n",
    "- Title\n",
    "- Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/properti/jakarta-timur/hos10008254/'\n",
    "browser.get(URL)\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Rumah Murah di Jalan Cipinang Baru Raya Jakarta Timur',\n",
       " 'price_currency': 'Rp',\n",
       " 'price_value': '2',\n",
       " 'price_unit': 'Miliar',\n",
       " 'address': 'Rawamangun, Jakarta Timur'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_class_name = 'ui-container.ui-property-page__main-container'\n",
    "header_element = browser.find_element_by_class_name(header_class_name)\n",
    "soup_header = BeautifulSoup(header_element.get_attribute('innerHTML'), 'html.parser')\n",
    "# scraping\n",
    "try: \n",
    "    currency, price, price_unit = \\\n",
    "        soup_header.select('.r123-listing-summary__price')[0].text.split()\n",
    "    title = soup_header.select('.r123-listing-summary__header-container-title')[0].text.strip()\n",
    "    address = soup_header.select('.r123-listing-summary__header-container-address')[0].text.strip()\n",
    "except AttributeError:\n",
    "    pass\n",
    "# compile header\n",
    "site_url = {'url': URL}\n",
    "header = dict(\n",
    "    title = title,\n",
    "    price_currency = currency,\n",
    "    price_value = price,\n",
    "    price_unit = price_unit,\n",
    "    address = address\n",
    ")\n",
    "\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Property Specifications**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed list of property specification will popped-up after we click *`menampilkan lebih banyak`*, as shown in below snapshot. We will retrieve the element after we *clicking* using the `WebDriver`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![popup](Scraped_Data/pic/5_Listing_Popup_Button.png)\n",
    "\n",
    " *<center> Listing specification </center>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commanding clicking\n",
    "details_popup_class_name = 'relative.ui-content-half__selector'\n",
    "click_element = browser.find_element_by_class_name(details_popup_class_name)\n",
    "browser.execute_script('arguments[0].click();', click_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K. Tidur': '3',\n",
       " 'K. Mandi': '3',\n",
       " 'L. Tanah': '56 m²',\n",
       " 'L. Bangunan': '87 m²',\n",
       " 'Carport': '1',\n",
       " 'Tipe Properti': 'Rumah',\n",
       " 'Sertifikat': 'SHM - Sertifikat Hak Milik',\n",
       " 'Daya Listrik': '2200 mAh',\n",
       " 'Jumlah Lantai': '2',\n",
       " 'Tahun dibangun': '2022',\n",
       " 'Kondisi Properti': 'Baru',\n",
       " 'Kondisi Perabotan': 'Unfurnished',\n",
       " 'ID Iklan': 'hos10008254'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_element = browser.find_element_by_class_name('ui-listing-specification__table')\n",
    "details_soup = BeautifulSoup(details_element.get_attribute('innerHTML'), 'html.parser')\n",
    "# compile available details\n",
    "details = {}\n",
    "for spec in details_soup.select('.ui-listing-specification__table--row'):\n",
    "    label, value = [_.text for _ in spec.find_all('p')]\n",
    "    details.update({label: value})\n",
    "\n",
    "details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the page doesn't have that clickable element (i.e. lister only provided minimum information of property specification), we just scrap from what is just available in the page, using below codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'K. Tidur': '3',\n",
       " 'K. Mandi': '3',\n",
       " 'L. Tanah': '56 m²',\n",
       " 'L. Bangunan': '87 m²',\n",
       " 'Carport': '1',\n",
       " 'Tipe Properti': 'Rumah',\n",
       " 'Sertifikat': 'SHM - Sertifikat Hak Milik'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_element = browser.find_element_by_class_name('ui-listing-specification__badge-wrapper')\n",
    "details_soup = BeautifulSoup(details_element.get_attribute('innerHTML'), 'html.parser')\n",
    "# compile available details\n",
    "details = {}\n",
    "for spec in details_soup.select('.ui-atomic-badges__children.relative'):\n",
    "    label, value = [_.text for _ in spec.find_all('p')]\n",
    "    details.update({label: value})\n",
    "\n",
    "details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Provided Facilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![facilities](Scraped_Data/pic/6_Facilities.png)\n",
    "\n",
    " *<center> Provided facilities </center>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'facilities': 'Keamanan, Keamanan 24 jam, CCTV'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facilities_element = browser.find_element_by_class_name('ui-facilities-portal__item-wrapper')\n",
    "facilities_soup = BeautifulSoup(facilities_element.get_attribute('innerHTML'), 'html.parser')\n",
    "facilities = {\n",
    "    'facilities': ', '.join({_.text for _ in facilities_soup.select('.ui-facilities-portal__item')})\n",
    "}\n",
    "browser.quit()\n",
    "\n",
    "facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Combining Scraped Data from a Single Listing Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From section `2.2 Accessing specific House Listing`, we have compiled several collections of scraped data as below:\n",
    "\n",
    "1. Records of **Listing URL**, stored in `url` variable \n",
    "2. Records of **Listing Header**, stored in `header` variable\n",
    "3. Records of **Property Details**, stored in `details` variable\n",
    "4. Records of **Provided Facilities**, stored in `facilities` variable\n",
    "\n",
    "Thus, a single observation of house listing will be presented as a `dictionary` of records, as explained in code below. We specifically use the dict `update` method to merge the records and ignoring the duplicated records (if any).\n",
    "\n",
    "Collection of observations will be stored as a **list of dictionaries**, which then can be processed using `pandas.DataFrame` constructor. More of this will be explained in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Completing Web Scraping Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now begin to complete our framework by creating a program to iterate over listing pages.\n",
    "\n",
    "The concepts of my `web scraping object` are:\n",
    "\n",
    "1. First thing to do is to collect the navigation links to each listing, which we obtain from `main entry page`, so we will create a non-public attribute to store these links and reset the attribute after all of the stored links have been accessed (a chance to preserve memory). Proceed to store new links for each main entry page.\n",
    "2. Scraping the data will be isolated by a single method, which stored in a `WebElement`. Each records that scraped will be passed to a `container attribute`. This method will be repetitively executed as the navigation links is passed.\n",
    "3. Provide a non-confusing `public method` to begin the scrap, using `starting url` and `page range`. The convenient exception handling and logging will be controlled in this method as the iteration of scraping methods (1 and 2) is continuously running to maintain the web scraping flow.\n",
    "4. Caching the scraped data after finished accessing a single main entry page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException, TimeoutException, StaleElementReferenceException         \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "class WebScraper():\n",
    "    \"\"\" Scrap the website using WebsScraper.scrap() method. Start URL should has '?' (means starts the query)\"\"\"\n",
    "    def __init__(self, start_url):\n",
    "        self._URL = start_url\n",
    "        self._nav_links = []\n",
    "        self._observations = []\n",
    "        self._chrome_options = webdriver.ChromeOptions()\n",
    "        self._chrome_options.add_argument('--disable-infobars')\n",
    "        self._chrome_options.add_argument('start-maximized')\n",
    "        self._chrome_options.add_argument('--disable-extensions')\n",
    "        self._chrome_options.add_argument('--disable-notifications')\n",
    "        self._prefs = {\"profile.default_content_setting_values.notifications\": 2}\n",
    "        self._chrome_options.add_experimental_option('prefs', self._prefs)\n",
    "        self._browser = webdriver.Chrome(\n",
    "            executable_path='D:\\\\chromedriver.exe',\n",
    "            options=self._chrome_options\n",
    "            )\n",
    "        \n",
    "    def _main_page_accessor(self, url):\n",
    "        \"\"\" Collects navigation links from a single entry page\"\"\"\n",
    "        url_ = url\n",
    "        logging.info(f'Scrapping {url_}')  \n",
    "        # Starts page access\n",
    "        self._browser.get(url_)\n",
    "        # Post access\n",
    "        main_listing_class_ = 'ui-organism-intersection__element.intersection-card-container'\n",
    "        try:\n",
    "            WebDriverWait(self._browser, 20).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, main_listing_class_))\n",
    "            )\n",
    "        finally:\n",
    "            main_listings_ = self._browser.find_elements_by_class_name(main_listing_class_)\n",
    "        links_ = []\n",
    "        for listing_ in main_listings_:\n",
    "            try: \n",
    "                soup_ = BeautifulSoup(listing_.get_attribute('innerHTML'), 'html.parser')\n",
    "                listing_class_ = 'ui-organisms-card-r123-featured__middle-section__title'\n",
    "                links_.append(\n",
    "                    'https://www.rumah123.com' + soup_.select(('.' + listing_class_))[0]['href']\n",
    "                )\n",
    "            except IndexError:\n",
    "                logging.warning(f'Failed to get (1) listing navigation link from {url_}')\n",
    "        self._nav_links = links_.copy()\n",
    "        time.sleep(3)\n",
    "\n",
    "    def _listing_scraper(self, url):\n",
    "        \"\"\" Scraps from a single listing. Observations is stored as a dictionary\"\"\"\n",
    "        url_ = url\n",
    "        scrap_data_ = {}\n",
    "        # Starts page access\n",
    "        self._browser.get(url_)\n",
    "        # Post access:\n",
    "        # 1. Scraping Header\n",
    "        try: \n",
    "            WebDriverWait(self._browser, 20).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'ui-listing-specification__badge-wrapper'))\n",
    "            )\n",
    "        finally:\n",
    "            header_class_name_ = 'ui-container.ui-property-page__main-container'\n",
    "            header_element_ = self._browser.find_element_by_class_name(header_class_name_)\n",
    "        try:\n",
    "            soup_header_ = BeautifulSoup(header_element_.get_attribute('innerHTML'), 'html.parser')\n",
    "            scrap_data_['url'] = url_\n",
    "            scrap_data_['currency'], scrap_data_['price'], scrap_data_['price_unit_scale'] = \\\n",
    "                soup_header_.select('.r123-listing-summary__price')[0].text.split()\n",
    "            scrap_data_['title'] = soup_header_.select('.r123-listing-summary__header-container-title')[0].text.strip()\n",
    "            scrap_data_['address'] = soup_header_.select('.r123-listing-summary__header-container-address')[0].text.strip()\n",
    "        except AttributeError:\n",
    "            logging.warning(\n",
    "                f'Failed to retrive property header for url ({url_}),'\n",
    "                ' may retrieve incomplete information')\n",
    "        # 2. Scraping Provided Facilities\n",
    "        try:\n",
    "            facilities_class_name_ = 'ui-facilities-portal__item-wrapper'\n",
    "            facilities_element_ = self._browser.find_element_by_class_name(facilities_class_name_)\n",
    "            facilities_soup_ = BeautifulSoup(facilities_element_.get_attribute('innerHTML'), 'html.parser')        \n",
    "            scrap_data_['facilities'] = \\\n",
    "                ', '.join({_.text for _ in facilities_soup_.select('.ui-facilities-portal__item')})\n",
    "            self._observations.append(scrap_data_)\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        # 3. Scraping Property Specification\n",
    "        details_popup_class_name_ = 'relative.ui-content-half__selector'\n",
    "        try:\n",
    "            #self._browser.find_element_by_class_name(details_popup_class_name_).click()\n",
    "            click_element_ = self._browser.find_element_by_class_name(details_popup_class_name_)\n",
    "            self._browser.execute_script('arguments[0].click();', click_element_)\n",
    "            details_element_ = self._browser.find_element_by_class_name('ui-listing-specification__table')\n",
    "            details_class_name_ = '.ui-listing-specification__table--row'\n",
    "        except (NoSuchElementException, ElementClickInterceptedException):\n",
    "            logging.warning(f'Click element not found for url ({url_}), basic information will be retrieved')\n",
    "            details_element_ = self._browser.find_element_by_class_name('ui-listing-specification__badge-wrapper')\n",
    "            details_class_name_ = '.ui-atomic-badges__children.relative'\n",
    "        try:\n",
    "            details_soup_ = BeautifulSoup(details_element_.get_attribute('innerHTML'), 'html.parser')\n",
    "            for spec in details_soup_.select(details_class_name_):\n",
    "                label_, value_ = [_.text.lower() for _ in spec.find_all('p')]\n",
    "                scrap_data_.update({label_: value_})\n",
    "        except AttributeError:\n",
    "            logging.warning(\n",
    "                f'Failed to retrive property specification for url ({url_}),'\n",
    "                ' may retrieve incomplete information')\n",
    "        time.sleep(3)\n",
    "    \n",
    "    def _cacher(self, file, until):\n",
    "        file_ = file\n",
    "        until_ = until\n",
    "        with open(file_, 'w', encoding='utf-8') as f:\n",
    "            f.write(f'Cached until {until_}\\n')\n",
    "            f.write(str(self._observations))\n",
    " \n",
    "    def scrap(self, start_page, end_page):\n",
    "        \"\"\" Scrap the website\"\"\"\n",
    "        current_date_time_ = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        logging.basicConfig(\n",
    "            filename=f'Scraped_Data/webscrap_log_{current_date_time_}.log',\n",
    "            format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "            level=logging.INFO \n",
    "            )\n",
    "        start_page_ = start_page\n",
    "        end_page_ = end_page\n",
    "        cache_file_path_ = f'Scraped_Data/temp_cache_{current_date_time_}.txt'\n",
    "        cache_file_ = open(cache_file_path_, 'a').close()\n",
    "        for page in range(start_page_, end_page_ + 1):\n",
    "            try: \n",
    "                entry_page_url_ = self._URL + f'&page={page}'  \n",
    "                self._main_page_accessor(entry_page_url_)\n",
    "                for link_ in self._nav_links:\n",
    "                    try:\n",
    "                        self._listing_scraper(link_)\n",
    "                    except TimeoutException:\n",
    "                        logging.warning(\n",
    "                            f'Timeout occured when accessing url ({link_}),'\n",
    "                            ' may retrieve incomplete information')\n",
    "                        continue\n",
    "                    except StaleElementReferenceException:  \n",
    "                        self._listing_scraper(link_)\n",
    "                try:\n",
    "                    self._cacher(cache_file_path_, page)\n",
    "                except UnicodeEncodeError:\n",
    "                    pass\n",
    "            except TimeoutException:\n",
    "                logging.warning(f'Timeout occured when accesing entry page ({page})')\n",
    "                self._nav_links = [] # resets the _nav_links, continue to next main entry page\n",
    "                continue\n",
    "            except KeyboardInterrupt:\n",
    "                logging.exception(f'KeyboardInterrupt captured while accessing page ({page})')\n",
    "                break\n",
    "        logging.info('Scrapping finished, quitting browser.')\n",
    "        self._browser.quit()\n",
    "        return self._observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try our scraping framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_URL = 'https://www.rumah123.com/jual/bogor/rumah/?'\n",
    "bogor_house_records = WebScraper(TEST_URL).scrap(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows an example of a single observation, a `dict` with its keys are `column field` and the values pair are the `records`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.rumah123.com/properti/bogor/hos10247521/#qid~76ff2ee6-b88b-4a4e-935d-8136ad1bd339',\n",
       " 'currency': 'Rp',\n",
       " 'price': '500',\n",
       " 'price_unit_scale': 'Juta',\n",
       " 'title': 'Rumah Strategis Nego di Palas Asri Town House Harga All In Siap Kpr',\n",
       " 'address': 'Bojong Gede, Bogor',\n",
       " 'facilities': 'Taman, Keamanan',\n",
       " 'k. tidur': '2',\n",
       " 'k. mandi': '1',\n",
       " 'l. tanah': '83 m²',\n",
       " 'l. bangunan': '45 m²',\n",
       " 'carport': '1',\n",
       " 'tipe properti': 'rumah',\n",
       " 'sertifikat': 'shm - sertifikat hak milik',\n",
       " 'daya listrik': '1300 mah',\n",
       " 'garasi': '1',\n",
       " 'jumlah lantai': '1',\n",
       " 'kondisi perabotan': 'unfurnished',\n",
       " 'id iklan': 'hos10247521'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bogor_house_records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we collected the observation, we are going to check if the records can be processed using `pandas.DataFrame`, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>url</th>\n",
       "      <td>https://www.rumah123.com/properti/bogor/hos106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currency</th>\n",
       "      <td>Rp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>1,68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_unit_scale</th>\n",
       "      <td>Miliar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Rumah Luas Strategis Dekat Cileungsi Akses Mud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address</th>\n",
       "      <td>Cileungsi, Bogor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facilities</th>\n",
       "      <td>Taman, Keamanan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k. tidur</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k. mandi</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l. tanah</th>\n",
       "      <td>200 m²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l. bangunan</th>\n",
       "      <td>90 m²</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carport</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tipe properti</th>\n",
       "      <td>rumah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sertifikat</th>\n",
       "      <td>hgb - hak guna bangunan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daya listrik</th>\n",
       "      <td>2200 mah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garasi</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jumlah lantai</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kondisi perabotan</th>\n",
       "      <td>unfurnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id iklan</th>\n",
       "      <td>hos10612778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kondisi properti</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hadap</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kt. pembantu</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>km. pembantu</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tahun dibangun</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land size</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building size</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carports</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>certificate</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maid bedrooms</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floors</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property condition</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furnishing</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building orientation</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     22\n",
       "url                   https://www.rumah123.com/properti/bogor/hos106...\n",
       "currency                                                             Rp\n",
       "price                                                              1,68\n",
       "price_unit_scale                                                 Miliar\n",
       "title                 Rumah Luas Strategis Dekat Cileungsi Akses Mud...\n",
       "address                                                Cileungsi, Bogor\n",
       "facilities                                              Taman, Keamanan\n",
       "k. tidur                                                              3\n",
       "k. mandi                                                              2\n",
       "l. tanah                                                         200 m²\n",
       "l. bangunan                                                       90 m²\n",
       "carport                                                             NaN\n",
       "tipe properti                                                     rumah\n",
       "sertifikat                                      hgb - hak guna bangunan\n",
       "daya listrik                                                   2200 mah\n",
       "garasi                                                                1\n",
       "jumlah lantai                                                         1\n",
       "kondisi perabotan                                           unfurnished\n",
       "id iklan                                                    hos10612778\n",
       "kondisi properti                                                    NaN\n",
       "hadap                                                               NaN\n",
       "kt. pembantu                                                          1\n",
       "km. pembantu                                                        NaN\n",
       "tahun dibangun                                                      NaN\n",
       "bedrooms                                                            NaN\n",
       "bathrooms                                                           NaN\n",
       "land size                                                           NaN\n",
       "building size                                                       NaN\n",
       "carports                                                            NaN\n",
       "certificate                                                         NaN\n",
       "electricity                                                         NaN\n",
       "maid bedrooms                                                       NaN\n",
       "floors                                                              NaN\n",
       "property condition                                                  NaN\n",
       "furnishing                                                          NaN\n",
       "building orientation                                                NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "house_bogor_df = pd.DataFrame(bogor_house_records)\n",
    "house_bogor_df.sample(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that information which not provided is labeled by `pandas` as missing, as we expected. But we suffer from inconsistent labeling (due to website html inconsistencies), for example `k. tidur` should has the same meaning with `bedrooms`. This can be easily cleaned using `pandas`, which I will explain in the next notebook.\n",
    "\n",
    "This conclude explanation about web scraping using `Selenium` and `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Obtaining Estimated Coordinate of Address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automating coordinate finding using gmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, NoSuchElementException  \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "import ast\n",
    "import re\n",
    "\n",
    "class CoordinateScraper():\n",
    "    def __init__(self):\n",
    "        self._coordinates = []\n",
    "        self._cached_index_until = 0\n",
    "        self._chrome_options = webdriver.ChromeOptions()\n",
    "        self._chrome_options.add_argument('--disable-infobars')\n",
    "        self._chrome_options.add_argument('start-maximized')\n",
    "        self._chrome_options.add_argument('--disable-extensions')\n",
    "        self._chrome_options.add_argument('--disable-notifications')\n",
    "        self._prefs = {\"profile.default_content_setting_values.notifications\": 2}\n",
    "        self._chrome_options.add_experimental_option('prefs', prefs)\n",
    "        self._browser = webdriver.Chrome(\n",
    "            executable_path='D:\\\\chromedriver.exe',\n",
    "            options=chrome_options\n",
    "            )\n",
    "    \n",
    "    def _location_scraper(self, keyword):\n",
    "        keyword_ = keyword\n",
    "        URL_ = 'https://www.google.com/maps/place/' + keyword_\n",
    "        self._browser.get(URL_)\n",
    "        time.sleep(5)\n",
    "        location_class_name_ = 'tAiQdd'\n",
    "        coordinate_class_name_ = 'gb_8d.gb_1.gb_8c'  \n",
    "        try: \n",
    "            WebDriverWait(self._browser, 20).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, location_class_name_))\n",
    "            )\n",
    "            WebDriverWait(self._browser, 20).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, coordinate_class_name_))\n",
    "            )\n",
    "        finally:\n",
    "            location_elements_ = self._browser.find_element_by_class_name(location_class_name_)\n",
    "        location_soup_ = BeautifulSoup(location_elements_.get_attribute('innerHTML'), 'html.parser')\n",
    "        location_tag_ = ', '.join(\n",
    "            [location_soup_.find_all('h1')[0].find('span').text]\\\n",
    "            + [loc_.find('span').text for loc_ in location_soup_.find_all('h2')]\n",
    "        )\n",
    "        if bool(location_tag_) and (len(location_tag_) > 3):\n",
    "            curr_url_ = self._browser.current_url\n",
    "            coord_ = re.findall('@.*/', curr_url_)[0][1:-1].split(sep=',')\n",
    "            lat_long_ = tuple(map(float, (coord_[0], coord_[1])))\n",
    "            self._coordinates.append({\n",
    "                'address': keyword_, 'lat_long': lat_long_, 'gmaps_tag': location_tag_\n",
    "                })\n",
    "        else:\n",
    "            logging.warning(f'No location matches for ({keyword_}), no coordinates returned')\n",
    "\n",
    "    def _cacher(self, file, start, until):\n",
    "        file_ = file\n",
    "        start_ = start\n",
    "        until_ = until\n",
    "        with open(file_, 'w', encoding='utf-8') as f:\n",
    "            f.write(f'Cached from {start_} until {until_}\\n')\n",
    "            f.write(str(self._coordinates))\n",
    "\n",
    "    def scrap_coordinates(self, locations:list, start_index:int):\n",
    "        start_ = start_index\n",
    "        locations_ = locations[start_:].copy()\n",
    "        current_date_time_ = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "        cache_file_path_ = f'Scraped_Data/coord_scrap_cache_{current_date_time_}.txt'\n",
    "        logging.basicConfig(\n",
    "            filename=f'Scraped_Data/coordscap_log_{current_date_time_}.log',\n",
    "            format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "            level=logging.INFO \n",
    "            )\n",
    "        for index, keyword in enumerate(locations_):\n",
    "            try:\n",
    "                self._location_scraper(keyword)\n",
    "            except (IndexError, NoSuchElementException):\n",
    "                logging.warning(f'No such element for ({locations_})')\n",
    "            except KeyboardInterrupt:\n",
    "                logging.exception(f'KeyboardInterrupt captured while accessing ({keyword})')\n",
    "                break\n",
    "            if (start_ + index) > (self._cached_index_until + start_ + 10):\n",
    "                self._cached_index_until += start_ + index\n",
    "                self._cacher(cache_file_path_, start_, self._cached_index_until)\n",
    "        logging.info('Scrapping finished, quitting browser.')\n",
    "        self._browser.quit()\n",
    "        return self._coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrap according to `location list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>lat_long</th>\n",
       "      <th>gmaps_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bekasi, Bekasi</td>\n",
       "      <td>(-6.2849775, 106.970127)</td>\n",
       "      <td>Kota Bks, Jawa Barat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Setu, Bekasi</td>\n",
       "      <td>(-6.358777, 107.0348331)</td>\n",
       "      <td>Kec. Setu, Kabupaten Bekasi, Jawa Barat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harapan Indah, Bekasi</td>\n",
       "      <td>(-6.1817523, 106.9736839)</td>\n",
       "      <td>Harapan Indah, RT.005/RW.010, Medan Satria, Ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bekasi Kota, Bekasi</td>\n",
       "      <td>(-6.2845395, 106.973377)</td>\n",
       "      <td>Bekasi, Kota Bks, Jawa Barat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cibitung, Bekasi</td>\n",
       "      <td>(-6.243801, 107.1036725)</td>\n",
       "      <td>Kec. Cibitung, Kabupaten Bekasi, Jawa Barat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 address                   lat_long  \\\n",
       "0         Bekasi, Bekasi   (-6.2849775, 106.970127)   \n",
       "1           Setu, Bekasi   (-6.358777, 107.0348331)   \n",
       "2  Harapan Indah, Bekasi  (-6.1817523, 106.9736839)   \n",
       "3    Bekasi Kota, Bekasi   (-6.2845395, 106.973377)   \n",
       "4       Cibitung, Bekasi   (-6.243801, 107.1036725)   \n",
       "\n",
       "                                           gmaps_tag  \n",
       "0                               Kota Bks, Jawa Barat  \n",
       "1            Kec. Setu, Kabupaten Bekasi, Jawa Barat  \n",
       "2  Harapan Indah, RT.005/RW.010, Medan Satria, Ke...  \n",
       "3                       Bekasi, Kota Bks, Jawa Barat  \n",
       "4        Kec. Cibitung, Kabupaten Bekasi, Jawa Barat  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Scraped_Data/raw/Location.txt') as f:\n",
    "    location_string = f.read()\n",
    "locations_list = ast.literal_eval(location_string)\n",
    "coordinates = CoordinateScraper().scrap_coordinates(locations_list, 0)\n",
    "pd.DataFrame(coordinates).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - Web Scraping Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping Jakarta House Price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import ast\n",
    "\n",
    "def to_csv(records, start_page, end_page, label):\n",
    "    current_date_time = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    pd.DataFrame(records).\\\n",
    "        to_csv(\n",
    "            f'Scraped_Data/{label}_{current_date_time}_page_{start_page}_to_{end_page}.csv',\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "def cached_records_to_list(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        cached_records = f.readlines()[1]\n",
    "    return ast.literal_eval(cached_records)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/jual/dki-jakarta/rumah/?'\n",
    "label = 'Jakarta'\n",
    "start_page = 1\n",
    "end_page = 10\n",
    "jakarta_records_1 = WebScraper(URL).scrap(start_page, end_page)\n",
    "to_csv(jakarta_records_1, start_page, end_page, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/jual/dki-jakarta/rumah/?'\n",
    "label = 'Jakarta'\n",
    "start_page = 11\n",
    "end_page = 20\n",
    "jakarta_records= WebScraper(URL).scrap(start_page, end_page)\n",
    "to_csv(jakarta_records, start_page, end_page, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/jual/dki-jakarta/rumah/?'\n",
    "label = 'Jakarta'\n",
    "start_page = 41\n",
    "end_page = 50\n",
    "jakarta_records= WebScraper(URL).scrap(start_page, end_page)\n",
    "to_csv(jakarta_records, start_page, end_page, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jakarta_records_21_37 = cached_records_to_list('Scraped_Data/temp_cache_2022-10-08_13-40.txt')\n",
    "to_csv(jakarta_records_21_37, 21, 37, 'Jakarta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping Bogor House Price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/jual/bogor/rumah/?'\n",
    "label = 'Bogor'\n",
    "start_page = 21\n",
    "end_page = 30\n",
    "bogor_records= WebScraper(URL).scrap(start_page, end_page)\n",
    "to_csv(bogor_records, start_page, end_page, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/jual/bogor/rumah/?'\n",
    "label = 'Bogor'\n",
    "start_page = 31\n",
    "end_page = 50\n",
    "bogor_records= WebScraper(URL).scrap(start_page, end_page)\n",
    "to_csv(bogor_records, start_page, end_page, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping Tangerang House Price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/jual/tangerang/rumah/?'\n",
    "label = 'Tangerang'\n",
    "start_page = 1\n",
    "end_page = 50\n",
    "tangerang_records= WebScraper(URL).scrap(start_page, end_page)\n",
    "to_csv(tangerang_records, start_page, end_page, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping Depok House Price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/jual/depok/rumah/?'\n",
    "label = 'Depok'\n",
    "start_page = 22\n",
    "end_page = 50\n",
    "depok_records= WebScraper(URL).scrap(start_page, end_page)\n",
    "to_csv(depok_records, start_page, end_page, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "depok_records_1_21 = cached_records_to_list('Scraped_Data/temp_cache_2022-10-09_11-37.txt')\n",
    "to_csv(depok_records_1_21, 1, 21, 'Depok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping Bekasi House Price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.rumah123.com/jual/bekasi/rumah/?'\n",
    "label = 'Bekasi'\n",
    "start_page = 1\n",
    "end_page = 50\n",
    "bekasi_records= WebScraper(URL).scrap(start_page, end_page)\n",
    "to_csv(bekasi_records, start_page, end_page, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f83cd4be805f1db6ae6750b283e805f338e0bedfaa45b40ed2173655a11de921"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
